{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc2hdd/home/hchen763/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/hpc2hdd/home/hchen763/.conda/envs/knowledgecircuit/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/hpc2hdd/home/hchen763/.conda/envs/knowledgecircuit/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "from eap.metrics import logit_diff, direct_logit\n",
    "import transformer_lens.utils as utils\n",
    "from eap.graph import Graph\n",
    "from eap.dataset import EAPDataset\n",
    "from eap.attribute import attribute\n",
    "import time\n",
    "from rich import print as rprint\n",
    "import pandas as pd\n",
    "from eap.evaluate import evaluate_graph, evaluate_baseline,get_circuit_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "LLAMA_2_7B_CHAT_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "from transformers import LlamaForCausalLM\n",
    "model = HookedTransformer.from_pretrained(LLAMA_2_7B_CHAT_PATH, device=\"cuda\", fold_ln=False, center_writing_weights=False, center_unembed=False)\n",
    "model.cfg.use_split_qkv_input = True\n",
    "model.cfg.use_attn_result = True\n",
    "model.cfg.use_hook_mlp_in = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_subject = 'Eiffel Tower'\n",
    "corrupted_subject = 'the Great Walls'\n",
    "clean = f'The official currency of the country where {clean_subject} is loacted in is the'\n",
    "corrupted = f'The official currency of the country where {corrupted_subject} is loacted in is the'\n",
    "assert len(model.to_str_tokens(clean.format(clean_subject))) == len(model.to_str_tokens(corrupted.format(corrupted_subject)))\n",
    "labels = ['Euro','Chinese']\n",
    "country_idx = model.tokenizer(labels[0],add_special_tokens=False).input_ids[0]\n",
    "corrupted_country_idx = model.tokenizer(labels[1],add_special_tokens=False).input_ids[0]\n",
    "# dataset = {k:[] for k in ['clean','country_idx', 'corrupted',  'corrupted_country_idx']}\n",
    "# for k, v in zip(['clean', 'country_idx', 'corrupted', 'corrupted_country_idx'], [clean, country_idx, corrupted, corrupted_country_idx]):\n",
    "#     dataset[k].append(v)\n",
    "# df2 = pd.DataFrame.from_dict(dataset)\n",
    "# df2.to_csv(f'capital_city.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [[country_idx, corrupted_country_idx]]\n",
    "label = torch.tensor(label)\n",
    "data = ([clean],[corrupted],label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = EAPDataset(filename='capital_city.csv',task='fact-retrieval')\n",
    "# dataloader = ds.to_dataloader(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "程序执行时间：65.61587119102478秒\n"
     ]
    }
   ],
   "source": [
    "g = Graph.from_model(model)\n",
    "start_time = time.time()\n",
    "# Attribute using the model, graph, clean / corrupted data and labels, as well as a metric\n",
    "attribute(model, g, data, partial(logit_diff, loss=True, mean=True), method='EAP-IG-case', ig_steps=100)\n",
    "# attribute(model, g, data, partial(direct_logit, loss=True, mean=True), method='EAP-IG-case', ig_steps=30)\n",
    "# attribute(model, g, dataloader, partial(logit_diff, loss=True, mean=True), method='EAP-IG', ig_steps=30)\n",
    "g.apply_topn(5000, absolute=True)\n",
    "g.prune_dead_nodes()\n",
    "\n",
    "g.to_json('graph.json')\n",
    "\n",
    "# gz = g.to_graphviz()\n",
    "# gz.draw(f'graph.png', prog='dot')\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"程序执行时间：{execution_time}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_component_logits(logits, model, answer_token, top_k=10):\n",
    "    logits = utils.remove_batch_dim(logits)\n",
    "    # print(heads_out[head_name].shape)\n",
    "    probs = logits.softmax(dim=-1)\n",
    "    token_probs = probs[-1]\n",
    "    answer_str_token = model.to_string(answer_token)\n",
    "    sorted_token_probs, sorted_token_values = token_probs.sort(descending=True)\n",
    "    # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n",
    "    correct_rank = torch.arange(len(sorted_token_values))[\n",
    "        (sorted_token_values == answer_token).cpu()\n",
    "    ].item()\n",
    "    # answer_ranks = []\n",
    "    # answer_ranks.append((answer_str_token, correct_rank))\n",
    "    # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n",
    "    # rprint gives rich text printing\n",
    "    print(\n",
    "        f\"Performance on answer token: Rank: {correct_rank: <8} Logit: {logits[-1, answer_token].item():5.2f} Prob: {token_probs[answer_token].item():6.2%} Token: |{answer_str_token}|\"\n",
    "    )\n",
    "    for i in range(top_k):\n",
    "        print(\n",
    "            f\"Top {i}th token. Logit: {logits[-1, sorted_token_values[i]].item():5.2f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{model.to_string(sorted_token_values[i])}|\"\n",
    "        )\n",
    "    # rprint(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on answer token: Rank: 0        Logit: 16.94 Prob: 56.56% Token: |Euro|\n",
      "Top 0th token. Logit: 16.94 Prob: 56.56% Token: |Euro|\n",
      "Top 1th token. Logit: 15.96 Prob: 21.39% Token: |French|\n",
      "Top 2th token. Logit: 14.06 Prob:  3.18% Token: |_|\n",
      "Top 3th token. Logit: 13.95 Prob:  2.85% Token: |euro|\n",
      "Top 4th token. Logit: 13.91 Prob:  2.74% Token: |Eu|\n"
     ]
    }
   ],
   "source": [
    "logits = get_circuit_logits(model, g, data)\n",
    "get_component_logits(logits, model, answer_token=model.to_tokens('Euro',prepend_bos=False)[0], top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = evaluate_baseline(model, dataloader, partial(logit_diff, loss=False, mean=False)).mean().item()\n",
    "results = evaluate_graph(model, g, dataloader, partial(logit_diff, loss=False, mean=False)).mean().item()\n",
    "print(f\"Original performance was {baseline}; the circuit's performance is {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/hpc2hdd/home/hchen763/jhaidata/local_model/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# 1. 加载分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant. You should generate answer as short as possible.<｜User｜>Natalia sold clips to 50 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Please reason step by step, and put the answer as short as possible.\n",
      "</think>\n",
      "\n",
      "Natalia sold 50 clips in April and half as many in May, which is 25 clips. In total, she sold 50 + 25 = 75 clips.  \n",
      "**Answer:** 75<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. You should generate answer as short as possible.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Natalia sold clips to 50 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "attention_mask = (inputs != tokenizer.pad_token_id).long()   # 通常如此，如果有 pad_token\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "response_ids = outputs[0]\n",
    "response = tokenizer.decode(response_ids)\n",
    "print(response)\n",
    "# print(response_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</think>\n",
      "\n",
      "Natalia sold clips in April to 50 friends and half as many in May. Therefore, she sold 25 clips in May. In total, she sold 50 + 25 = 75 clips in April and May.\n",
      "\n",
      "Answer: \\boxed{75}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    <｜begin▁of▁sentence｜>You are a helpful assistant. You should generate answer as short as possible.\n",
    "    <｜User｜>Natalia sold clips to 50 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
    "\"\"\"\n",
    "\n",
    "# 编码，推理\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:   1%|          | 1/100 [00:05<08:47,  5.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:   2%|▏         | 2/100 [00:10<08:40,  5.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:   3%|▎         | 3/100 [00:16<08:39,  5.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:   4%|▍         | 4/100 [00:19<07:26,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:   5%|▌         | 5/100 [00:25<07:49,  4.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:   6%|▌         | 6/100 [00:30<07:59,  5.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:   7%|▋         | 7/100 [00:35<08:03,  5.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:   8%|▊         | 8/100 [00:41<08:05,  5.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:   9%|▉         | 9/100 [00:46<08:07,  5.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  10%|█         | 10/100 [00:52<08:06,  5.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  11%|█         | 11/100 [00:57<08:05,  5.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  12%|█▏        | 12/100 [01:03<07:54,  5.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  13%|█▎        | 13/100 [01:08<07:49,  5.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  14%|█▍        | 14/100 [01:13<07:44,  5.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  15%|█▌        | 15/100 [01:19<07:38,  5.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  16%|█▌        | 16/100 [01:24<07:30,  5.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  17%|█▋        | 17/100 [01:29<07:23,  5.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  18%|█▊        | 18/100 [01:35<07:17,  5.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  19%|█▉        | 19/100 [01:40<07:14,  5.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  20%|██        | 20/100 [01:46<07:12,  5.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  21%|██        | 21/100 [01:51<07:08,  5.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  22%|██▏       | 22/100 [01:56<06:59,  5.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  23%|██▎       | 23/100 [02:02<06:51,  5.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  24%|██▍       | 24/100 [02:07<06:39,  5.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  25%|██▌       | 25/100 [02:12<06:35,  5.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  26%|██▌       | 26/100 [02:17<06:30,  5.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  27%|██▋       | 27/100 [02:23<06:25,  5.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  28%|██▊       | 28/100 [02:28<06:21,  5.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  29%|██▉       | 29/100 [02:34<06:29,  5.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  30%|███       | 30/100 [02:47<09:08,  7.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  31%|███       | 31/100 [02:54<08:33,  7.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  32%|███▏      | 32/100 [02:59<07:39,  6.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  33%|███▎      | 33/100 [03:04<06:54,  6.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  34%|███▍      | 34/100 [03:09<06:29,  5.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  35%|███▌      | 35/100 [03:12<05:19,  4.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  36%|███▌      | 36/100 [03:17<05:20,  5.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  37%|███▋      | 37/100 [03:22<05:19,  5.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  38%|███▊      | 38/100 [03:27<05:17,  5.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  39%|███▉      | 39/100 [03:33<05:14,  5.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  40%|████      | 40/100 [03:38<05:10,  5.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  41%|████      | 41/100 [03:43<05:05,  5.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  42%|████▏     | 42/100 [03:48<05:02,  5.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  43%|████▎     | 43/100 [03:53<04:56,  5.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  44%|████▍     | 44/100 [03:59<04:52,  5.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  45%|████▌     | 45/100 [04:04<04:48,  5.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  46%|████▌     | 46/100 [04:09<04:44,  5.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  47%|████▋     | 47/100 [04:14<04:28,  5.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  48%|████▊     | 48/100 [04:19<04:26,  5.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  49%|████▉     | 49/100 [04:24<04:23,  5.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  50%|█████     | 50/100 [04:30<04:20,  5.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  51%|█████     | 51/100 [04:35<04:16,  5.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  52%|█████▏    | 52/100 [04:40<04:12,  5.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  53%|█████▎    | 53/100 [04:46<04:07,  5.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  54%|█████▍    | 54/100 [04:51<04:02,  5.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  55%|█████▌    | 55/100 [04:56<03:57,  5.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  56%|█████▌    | 56/100 [05:01<03:41,  5.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  57%|█████▋    | 57/100 [05:05<03:25,  4.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  58%|█████▊    | 58/100 [05:10<03:27,  4.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  59%|█████▉    | 59/100 [05:14<03:13,  4.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  60%|██████    | 60/100 [05:20<03:16,  4.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  61%|██████    | 61/100 [05:25<03:16,  5.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  62%|██████▏   | 62/100 [05:28<02:52,  4.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  63%|██████▎   | 63/100 [05:34<02:56,  4.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  64%|██████▍   | 64/100 [05:39<02:57,  4.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  65%|██████▌   | 65/100 [05:44<02:54,  4.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  66%|██████▌   | 66/100 [05:49<02:52,  5.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  67%|██████▋   | 67/100 [05:55<02:49,  5.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  68%|██████▊   | 68/100 [06:00<02:46,  5.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  69%|██████▉   | 69/100 [06:05<02:41,  5.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  70%|███████   | 70/100 [06:08<02:13,  4.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  71%|███████   | 71/100 [06:13<02:16,  4.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  72%|███████▏  | 72/100 [06:18<02:11,  4.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  73%|███████▎  | 73/100 [06:23<02:11,  4.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  74%|███████▍  | 74/100 [06:29<02:10,  5.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  75%|███████▌  | 75/100 [06:34<02:07,  5.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  76%|███████▌  | 76/100 [06:39<02:04,  5.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  77%|███████▋  | 77/100 [06:45<02:00,  5.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  78%|███████▊  | 78/100 [06:50<01:55,  5.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  79%|███████▉  | 79/100 [06:55<01:51,  5.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  80%|████████  | 80/100 [07:00<01:44,  5.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  81%|████████  | 81/100 [07:06<01:39,  5.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  82%|████████▏ | 82/100 [07:11<01:35,  5.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  83%|████████▎ | 83/100 [07:16<01:30,  5.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  84%|████████▍ | 84/100 [07:21<01:23,  5.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  85%|████████▌ | 85/100 [07:27<01:18,  5.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  86%|████████▌ | 86/100 [07:32<01:13,  5.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  87%|████████▋ | 87/100 [07:36<01:02,  4.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  88%|████████▊ | 88/100 [07:41<00:59,  4.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  89%|████████▉ | 89/100 [07:46<00:55,  5.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  90%|█████████ | 90/100 [07:50<00:47,  4.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  91%|█████████ | 91/100 [07:54<00:39,  4.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  92%|█████████▏| 92/100 [07:59<00:37,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  93%|█████████▎| 93/100 [08:05<00:34,  4.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  94%|█████████▍| 94/100 [08:10<00:30,  5.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  95%|█████████▌| 95/100 [08:15<00:25,  5.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  96%|█████████▌| 96/100 [08:21<00:20,  5.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  97%|█████████▋| 97/100 [08:24<00:13,  4.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  98%|█████████▊| 98/100 [08:29<00:09,  4.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating:  99%|█████████▉| 99/100 [08:34<00:04,  4.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating: 100%|██████████| 100/100 [08:40<00:00,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on first 100 GSM8K dev samples: 25.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 显卡1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 本地模型路径\n",
    "model_path = \"/hpc2hdd/home/hchen763/jhaidata/local_model/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# 载入分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "# 载入GSM8K dev集\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "def make_chat(question):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. You should generate answer as short as possible.\"},\n",
    "        {\"role\": \"user\", \"content\": question.strip()}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def extract_number(ans):\n",
    "    import re\n",
    "    numbers = re.findall(r\"[-+]?\\d*\\.?\\d+\", ans)\n",
    "    return numbers[-1] if numbers else ans.strip()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for sample in tqdm(dataset.select(range(100)), desc=\"Evaluating\"):\n",
    "    q = sample[\"question\"]\n",
    "    gt = sample[\"answer\"]\n",
    "    prompt = make_chat(q)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    gen = tokenizer.decode(output[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    pred_ans = extract_number(gen)\n",
    "    gt_ans = extract_number(gt)\n",
    "    if pred_ans == gt_ans:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "acc = correct / total\n",
    "print(f\"Accuracy on first 100 GSM8K dev samples: {acc:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledgecircuit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
