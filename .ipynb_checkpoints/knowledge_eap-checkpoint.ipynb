{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functools import partial\n",
    "from rich import print as rprint\n",
    "from transformers import LlamaForCausalLM\n",
    "from eap.graph import Graph\n",
    "from eap.dataset import EAPDataset\n",
    "from eap.attribute import attribute\n",
    "from eap.metrics import logit_diff, direct_logit\n",
    "from eap.evaluate import evaluate_graph, evaluate_baseline,get_circuit_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\"/hpc2hdd/home/hchen763/jhaidata/local_model/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"/hpc2hdd/home/hchen763/jhaidata/local_model/DeepSeek-R1-Distill-Qwen-1.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_2_7B_CHAT_PATH = \"Qwen/Qwen2-1.5B\"\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(LLAMA_2_7B_CHAT_PATH, hf_model=hf_model, device=\"cuda\", fold_ln=False, center_writing_weights=False, center_unembed=False)\n",
    "model.cfg.use_split_qkv_input = False\n",
    "model.cfg.use_attn_result = True\n",
    "model.cfg.use_hook_mlp_in = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_subject = 'the Eiffel Tower'\n",
    "corrupted_subject = 'the the the Great Walls'\n",
    "clean = f'The official currency of the country where {clean_subject} is loacted in is the'\n",
    "corrupted = f'The official currency of the country where {corrupted_subject} is loacted in is the'\n",
    "\n",
    "assert len(model.to_str_tokens(clean.format(clean_subject))) == len(model.to_str_tokens(corrupted.format(corrupted_subject)))\n",
    "\n",
    "labels = ['Euro','Chinese']\n",
    "country_idx = model.tokenizer(labels[0],add_special_tokens=False).input_ids[0]\n",
    "corrupted_country_idx = model.tokenizer(labels[1],add_special_tokens=False).input_ids[0]\n",
    "# dataset = {k:[] for k in ['clean','country_idx', 'corrupted',  'corrupted_country_idx']}\n",
    "# for k, v in zip(['clean', 'country_idx', 'corrupted', 'corrupted_country_idx'], [clean, country_idx, corrupted, corrupted_country_idx]):\n",
    "#     dataset[k].append(v)\n",
    "# df2 = pd.DataFrame.from_dict(dataset)\n",
    "# df2.to_csv(f'capital_city.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_subject = '48'\n",
    "corrupted_subject = '50'\n",
    "clean = f'Natalia sold clips to {clean_subject} of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?'\n",
    "corrupted = f'Natalia sold clips to {corrupted_subject} of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?'\n",
    "\n",
    "def pad_to_same_token_length(s1, s2, model):\n",
    "    # 不断补空格，直到token长度一样\n",
    "    while len(model.to_str_tokens(s1)) < len(model.to_str_tokens(s2)):\n",
    "        s1 += ' '\n",
    "    while len(model.to_str_tokens(s2)) < len(model.to_str_tokens(s1)):\n",
    "        s2 += ' '\n",
    "    return s1, s2\n",
    "\n",
    "# 补齐token长度\n",
    "clean, corrupted = pad_to_same_token_length(clean, corrupted, model)\n",
    "\n",
    "# 这里可选，assert确保token长度一致\n",
    "assert len(model.to_str_tokens(clean)) == len(model.to_str_tokens(corrupted))\n",
    "\n",
    "labels = ['Euro','Chinese']\n",
    "country_idx = model.tokenizer(labels[0],add_special_tokens=False).input_ids[0]\n",
    "corrupted_country_idx = model.tokenizer(labels[1],add_special_tokens=False).input_ids[0]\n",
    "\n",
    "label = [[country_idx, corrupted_country_idx]]\n",
    "label = torch.tensor(label)\n",
    "\n",
    "data = ([clean], [corrupted], label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [[country_idx, corrupted_country_idx]]\n",
    "label = torch.tensor(label)\n",
    "data = ([clean],[corrupted],label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = EAPDataset(filename='capital_city.csv',task='fact-retrieval')\n",
    "# dataloader = ds.to_dataloader(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph.from_model(model)\n",
    "start_time = time.time()\n",
    "# Attribute using the model, graph, clean / corrupted data and labels, as well as a metric\n",
    "attribute(model, g, data, partial(logit_diff, loss=True, mean=True), method='EAP-IG-case', ig_steps=100)\n",
    "# attribute(model, g, data, partial(direct_logit, loss=True, mean=True), method='EAP-IG-case', ig_steps=30)\n",
    "# attribute(model, g, dataloader, partial(logit_diff, loss=True, mean=True), method='EAP-IG', ig_steps=30)\n",
    "g.apply_topn(5000, absolute=True)\n",
    "g.prune_dead_nodes()\n",
    "\n",
    "g.to_json('graph.json')\n",
    "\n",
    "gz = g.to_graphviz()\n",
    "gz.draw(f'graph.png', prog='dot')\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"程序执行时间：{execution_time}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_component_logits(logits, model, answer_token, top_k=10):\n",
    "    logits = utils.remove_batch_dim(logits)\n",
    "    # print(heads_out[head_name].shape)\n",
    "    probs = logits.softmax(dim=-1)\n",
    "    token_probs = probs[-1]\n",
    "    answer_str_token = model.to_string(answer_token)\n",
    "    sorted_token_probs, sorted_token_values = token_probs.sort(descending=True)\n",
    "    # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n",
    "    correct_rank = torch.arange(len(sorted_token_values))[\n",
    "        (sorted_token_values == answer_token).cpu()\n",
    "    ].item()\n",
    "    # answer_ranks = []\n",
    "    # answer_ranks.append((answer_str_token, correct_rank))\n",
    "    # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n",
    "    # rprint gives rich text printing\n",
    "    rprint(\n",
    "        f\"Performance on answer token:\\n[b]Rank: {correct_rank: <8} Logit: {logits[-1, answer_token].item():5.2f} Prob: {token_probs[answer_token].item():6.2%} Token: |{answer_str_token}|[/b]\"\n",
    "    )\n",
    "    for i in range(top_k):\n",
    "        print(\n",
    "            f\"Top {i}th token. Logit: {logits[-1, sorted_token_values[i]].item():5.2f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{model.to_string(sorted_token_values[i])}|\"\n",
    "        )\n",
    "    # rprint(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = get_circuit_logits(model, g, data)\n",
    "get_component_logits(logits, model, answer_token=model.to_tokens('Euro',prepend_bos=False)[0], top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = evaluate_baseline(model, dataloader, partial(logit_diff, loss=False, mean=False)).mean().item()\n",
    "results = evaluate_graph(model, g, dataloader, partial(logit_diff, loss=False, mean=False)).mean().item()\n",
    "print(f\"Original performance was {baseline}; the circuit's performance is {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/hpc2hdd/home/hchen763/jhaidata/local_model/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# 1. 加载分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Please reason step by step, and put the answer as short as possible.\n",
      "</think>\n",
      "\n",
      "Natalia sold 50 clips in April and half as many in May, which is 25 clips. In total, she sold 50 + 25 = 75 clips.  \n",
      "**Answer:** 75\n",
      "tensor([  5209,   2874,   3019,    553,   3019,     11,    323,   2182,    279,\n",
      "          4226,    438,   2805,    438,   3204,    624, 151649,    271,     45,\n",
      "          4212,    685,   6088,    220,     20,     15,  26111,    304,   5813,\n",
      "           323,   4279,    438,   1657,    304,   3217,     11,    892,    374,\n",
      "           220,     17,     20,  26111,     13,    758,   2790,     11,   1340,\n",
      "          6088,    220,     20,     15,    488,    220,     17,     20,    284,\n",
      "           220,     22,     20,  26111,     13,   2303,    334,  16141,  66963,\n",
      "           220,     22,     20, 151643], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. You should generate answer as short as possible.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Natalia sold clips to 50 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "attention_mask = (inputs != tokenizer.pad_token_id).long()   # 通常如此，如果有 pad_token\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "response_ids = outputs[0][inputs.shape[-1]:]\n",
    "response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "print(response)\n",
    "print(response_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</think>\n",
      "\n",
      "Natalia sold clips in April to 50 friends, and in May she sold half as many clips. To find the total number of clips sold in April and May, we calculate:\n",
      "\n",
      "- April: 50 clips\n",
      "- May: \\( \\frac{50}{2} = 25 \\) clips\n",
      "\n",
      "Total: \\( 50 + 25 = 75 \\) clips\n",
      "\n",
      "So, Natalia sold a total of **75 clips**\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    <｜begin▁of▁sentence｜>You are a helpful assistant. You should generate answer as short as possible.\n",
    "    <｜User｜>Natalia sold clips to 50 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
    "\"\"\"\n",
    "\n",
    "# 编码，推理\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledgecircuit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
